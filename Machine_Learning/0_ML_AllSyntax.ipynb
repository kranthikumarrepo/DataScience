{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626d69e2-cf35-48f3-a0f5-a0d0c338c53c",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c4228c8-fa3f-42e3-b766-5538273e8137",
   "metadata": {},
   "source": [
    "y = mx + c\n",
    "\n",
    "y (Predicted Value) = m(slope) * x (Input Value) + C (Y Intercept)\n",
    "\n",
    "#### LinReGress - To get slope and intercept values\n",
    "- Slope(y), Intercept(c) is calculated by using the linregress function.\n",
    "- slope, intercept, r, p, std_err = stats.linregress(x, y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b86bee79-5c64-4e48-9b68-eb5f0b5cfe71",
   "metadata": {},
   "source": [
    "### Cost Functions - Find the best fit lines for given training data set.\n",
    "- Mean Absolute Error - MAE - (Sum of |Actual Value - Predicted Value|)/N >>> (Sum of |Actual Value - (mx + b)|)/N\n",
    "- Mean Square Error - MSE - (Square of Actual Value - Predicted Value)/N >>> (Square of (Actual Value - (mx + b)))/N\n",
    "- Root Mean Square Error - RMSE - Sq Root(Square of Actual Value - Predicted Value)/N) >>> Sq Root((Square of (Actual Value - (mx + b)))/N)\n",
    "- R2 Score = (Variance(Mean Line) - Variance(Linear Regression Line)) / Variance(Mean Line)\n",
    "\n",
    "Python Syntax:-\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "MAE_Formula = mean_absolute_error(df_y_test_pkg, df_y_predict_pkg)\n",
    "mse_value = mean_squared_error(df_y_test_pkg, df_y_predict_pkg)\n",
    "RMSE = root_mean_squared_error(df_y_test_pkg, df_y_predict_pkg)\n",
    "R2Score = r2_score(df_y_test_pkg, df_y_predict_pkg)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2958c422-18a5-4367-bc42-89965e905275",
   "metadata": {},
   "source": [
    "### All Linear Regression Steps at once place\n",
    "\n",
    "#### To Train the data:-\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "my_model = LinearRegression()\n",
    "\n",
    "my_model.fit(x_train_cgpa,y_train_pkg)\n",
    "\n",
    "#### To Predict the outcome:-\n",
    "y_predict_sales = my_model.predict(x_testTVRadioNews)\n",
    "\n",
    "# y = mx +c\n",
    "# slope m, value\n",
    "my_model.coef_\n",
    "\n",
    "# intercept c, Value\n",
    "my_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd46a3b-7147-4f0e-ae7e-e5cc780c5a70",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2871729-b388-4758-96f8-757b9959dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "- sigmoid(z) = 1/(1 + e power -z), where e = Eulers number = 2.71828 >>> Outcome of Sigmoid function is always between 0 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0c1740c-9454-4f7e-8dbc-ea162ac68e87",
   "metadata": {},
   "source": [
    "### All Logistic Regression Steps at once place\n",
    "\n",
    "#### To Train the data:-\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "my_model = LogisticRegression()\n",
    "\n",
    "my_model.fit(x_train_cgpa,y_train_pkg)\n",
    "\n",
    "# Predict the outcome by giving test input data\n",
    "my_model.predict(x_test)\n",
    "\n",
    "# Probablity of yes or no for each test input data\n",
    "my_model.predict_proba(x_test)\n",
    "\n",
    "# Check the score - Accuracy of the model\n",
    "my_model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276002f7-e668-4f99-9220-c661fc4fbbe0",
   "metadata": {},
   "source": [
    "#### UnSupervised K Means Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f6cb879-e0ce-4d80-a5a7-0c10243c2e67",
   "metadata": {},
   "source": [
    "### All K Means Clustering Steps at once place\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "my_model = KMeans(n_clusters=3,random_state=42) >>> k = 3 in this case.\n",
    "\n",
    "# Train the model\n",
    "my_model.fit(x)\n",
    "\n",
    "# Predict the outcome\n",
    "my_model.labels_\n",
    "\n",
    "# To Train and Predict in one step\n",
    "my_model.fit_predict(x)\n",
    "\n",
    "# To find the centroids\n",
    "centroids = my_model.cluster_centers_\n",
    "\n",
    "# Calculate WCSS Score\n",
    "my_model.inertia_\n",
    "\n",
    "# Repeat the above steps by changing the k value and compare it with WCSS Score. \n",
    "# Draw elbox diagram (Line chart - Different K Values on X Axis and its respective WCSS Score on Y Axis) and decide the final value of k.\n",
    "# Rerun the model with the final k value and predict the values.\n",
    "\n",
    "#### WCSS Score - helps to determine the ideal k value by drawing the elbow graph. K Value on X axis and WCSS Score on Y Axis.\n",
    "# Inertia = WCSS Score calculated by Python - Measures how well a dataset was clustered by K-Means.It is calculated by measuring the\n",
    "# distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster.\n",
    "\n",
    "#### Silhouette_score - Helps to check the accuracy of the model. \n",
    "# Silhouette = (Outside Distance w.r.t. data point - Within Distance w.r.t. same data point) / Max(Outside Distance, Within Distance)\n",
    "# Ex1:- Wd = 0.2 and Od = 0.8 >>> (0.8 - 0.2)/Max(0.8,0.2) = 0.6/0.8 = 0.75 >>> Good for that datapoint\n",
    "# Ex1:- Wd = 0.8 and Od = 0.2 >>> (0.2 - 0.8)/Max(0.8,0.2) = -0.6/0.8 = -0.75 >>> Bad for that datapoint\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(x,y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a64753-9938-4d5e-826b-2b55cdd7f1f0",
   "metadata": {},
   "source": [
    "#### Common For All Algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04586579-eef2-464e-a8ab-a7c55275ce56",
   "metadata": {},
   "source": [
    "### Gradient Descent - Common across all algorithms\n",
    "- Check notes 3_MLNotes_GradientDescent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5216c984-ae28-4a3f-bf52-b2e25efe05aa",
   "metadata": {},
   "source": [
    "#### To Split the data:-\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_cgpa,x_test_cgpa,y_train_pkg,y_test_pkg = train_test_split(x_train_cgpa_data,y_train_package_data,test_size=0.2,random_state=42)\n",
    "- Note1:- If test size is not given, default is 75%\n",
    "- Note2:- Important Attributes Description\n",
    "- random_state = 42 >>> if we run multiple times, it rememebers the records seleccted for ID 42. As long as we keep random_state same, it selects the same records in the test sample.\n",
    "- test_size - Default value if test_size is not given = 75%"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be2bd375-a113-4112-b52f-4951e53d15cc",
   "metadata": {},
   "source": [
    "#### To Draw the plots\n",
    "\n",
    "Linear Regression:-\n",
    "\n",
    "plt.scatter(x_train_cgpa,y_train_pkg)\n",
    "plt.plot(x_test_cgpa,my_model.predict(x_test_cgpa))\n",
    "sns.regplot(x=x_train_cgpa_data, y=y_train_package_data)\n",
    "\n",
    "Logistic Regression:-\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1/(1 + np.exp(-x))\n",
    "mymodel = list(map(sigmoid, x))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, mymodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d2ba7-9c5b-4ce0-a5fd-7702bff22e65",
   "metadata": {},
   "source": [
    "#### Misc. Python Functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38bc73e6-bd6d-4c76-bf4b-42d7aefe6b3c",
   "metadata": {},
   "source": [
    "# To Reset Index, i.e. if the indexes are 0,2,5 after train test split >>> To change indexes to 0,1,2...\n",
    "df_y_test_pkg = df_y_test_pkg.reset_index(drop=True)\n",
    "\n",
    "# Rename column name\n",
    "df_y_predict_pkg.rename(columns={'predict': 'y_predict_pkg'}, inplace=True)\n",
    "\n",
    "# Sum and absolute functions\n",
    "sum(abs(df_test_package.y_predict_pkg - df_test_package.y_test_pkg))/50\n",
    "\n",
    "# To get index and column values\n",
    "df_y_test_pkg[['Column_Name']]\n",
    "\n",
    "# To get index and column values but not in dataframe format\n",
    "df_y_test_pkg.Column_Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de8652-03d5-4fa3-a5ea-b164899574b0",
   "metadata": {},
   "source": [
    "#### Feature Scaling - Numeric Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6e60a7b-85a0-4714-8e12-11c335781dbe",
   "metadata": {},
   "source": [
    "# Normalization (Min-Max Scaling):- Scales the data to a specific range, typically between 0 and 1.\n",
    "- from sklearn.preprocessing import MinMaxScaler\n",
    "- scaler = MinMaxScaler()\n",
    "- scaler.fit(data_df)\n",
    "- data_df = scaler.transform(data_df)\n",
    "- Or\n",
    "- data_df = scalar.fit_transform(data_df) >>> Train and Transform in one syntax\n",
    "- Scaled data (data_df) will be in the form of numpy array, convert it back to dataframe\n",
    "- data_df = pd.DataFrame(data_df)\n",
    "-\n",
    "# Standardization:- Scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "- from sklearn.preprocessing import StandardScaler\n",
    "- scaler = StandardScaler()\n",
    "- scaler.fit(data_df)\n",
    "- data_df = scaler.transform(data_df)\n",
    "- Or\n",
    "- data_df = scaler.fit_transform(data_df) >>> Train and Transform in one syntax\n",
    "- Scaled data (data_df) will be in the form of numpy array, convert it back to dataframe\n",
    "- data_df = pd.DataFrame(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba54bd-c8ee-41f7-81d8-85760fd33a27",
   "metadata": {},
   "source": [
    "#### Feature Encoding For Categorical Data :- OrdinalEncoder, OneHotEncoder & LabelEncoder\n",
    "#### Column Transformer\n",
    "#### Imputer - SimpleImputer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc7a0d89-cc2b-4276-ac1c-77201975b44f",
   "metadata": {},
   "source": [
    "# Impute nulls with Mean(Default), Median, Most Frequent, Constant\n",
    "from sklearn.impute import SimpleImputer\n",
    "SimpleImputerObj = SimpleImputer()\n",
    "x_train_fever = SimpleImputerObj.fit_transform(x_train[['fever']]) >>> Replace all the nulls in fever column with Mean\n",
    "\n",
    "# Ordinal Encoding - For Categorical, Ordinal Data > Specifically on X (input columns)\n",
    "Ex: Applied on one column >>> Cough column with mild and strong categories. Fit_Transform in a single line.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "OrdinalEncoderObj = OrdinalEncoder(categories=[['Mild','Strong']]) >>> Specify the Ordinal values in hierarcheal order in the required column.\n",
    "x_train_cough = OrdinalEncoderObj.fit_transform(x_train[['cough']]) >>> Specify the column name on which Ordinal encoding has to be applied.\n",
    "\n",
    "Ex: Applied on 2 columns(review and education) at once. \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "OrdinalEncoder_Obj = OrdinalEncoder(categories=[['Poor','Average','Good'],['School','UG','PG']])\n",
    "Fit and transform in single line.\n",
    "x_train_review_education = OrdinalEncoderObj.fit_transform(x_train[['review'],['education']])\n",
    "Fit and transform in different lines.\n",
    "OrdinalEncoder_Obj.fit(x_train[['review'],['education']])\n",
    "x_train_review_education = OrdinalEncoder_Obj.transform(x_train[['review'],['education']])\n",
    ">>> To check the categories / hirearchy given in the object :- OrdinalEncoder_Obj.categories >>> [['Poor', 'Average', 'Good'], ['School', 'UG', 'PG']]\n",
    "\n",
    "# OneHotEncoder - For Categorical, Nominal Data - using ML Sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OneHotEncoderObj = OneHotEncoder(drop='first',sparse_output=False) >>> drop='first' to perform k-1 dummy columns, if nothing is given, k dummy columns are created.\n",
    "x_train_gender_city = OneHotEncoderObj.fit_transform(x_train[['gender','city']]) >>> if gender has 3 categories >> 2 dummy columns, if city has 2 categories >>> 1 dummy column is created.\n",
    ">>> To check the categories :- OneHotEncoderObj.categories >>> 'auto'\n",
    "----------\n",
    "# OneHotEncoder - For Categorical, Nominal Data - using Python Pandas\n",
    "pd.get_dummies(DF, columns=['gender','city']) >>> It creates k dummy columns\n",
    "pd.get_dummies(DF, columns=['gender','city'],drop_first = True) >>> It creates k-1 dummy columns\n",
    "\n",
    "\n",
    "# Label Encoding - For Categorical data - Similar to ordinal encoding, but applied on output (Y) Data only.\n",
    "# If any one of the input columns(X) is Ordinal and if output label(Y) is categorical, we apply label encoding on output (Y), instead of ordinal encoding.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "LabelEncoder_Obj = LabelEncoder()\n",
    "LabelEncoder_Obj.fit(y_train)\n",
    "y_train = LabelEncoder_Obj.transform(y_train) >>> y_train has Purchased column which has values yes / no.\n",
    "LabelEncoder_Obj.classes_ >>> Tells how many unique output values are present in Purchased column(Yes / No)\n",
    "\n",
    "# To concatenate all the columns that were transformed seperately.\n",
    "x_train_transformed = np.concatenate((x_train_age,x_train_fever,x_train_gender_city,x_train_cough),axis=1)\n",
    "\n",
    "# Instead use columntransformer to do all the steps in one go.\n",
    "# from sklearn.compose import ColumnTransformer\n",
    ">>> Skeliton >>> ColumnTransformerObj = ColumnTransformer(transformers=[(,,[])],remainder='passthrough') >>> transformers = set of transformations like #SimpleImputer, OrdinalEncoder, OneHotEncoder, etc. and also give the column names on which those transformation needs to be applied. #remainder='passthrough' >>> Do not take any action on columns not mentioned in the dataframe.\n",
    "#dataframe name is mentioned while giving fit_transform.\n",
    "\n",
    "ColumnTransformerObj = ColumnTransformer(transformers=[\n",
    "            ('Transf1',SimpleImputer(),['fever']),\n",
    "            ('Transf2',OrdinalEncoder(categories=[['Mild','Strong']]),['cough']),\n",
    "            ('Transf3',OneHotEncoder(drop='first',sparse_output=False),['gender','city'])\n",
    "],remainder='passthrough')\n",
    "\n",
    "x_train_transformed = ColumnTransformerObj.fit_transform(x_train_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5ac12-29f5-4634-b120-7a9450c6b6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
